# Website Crawler

# üï∑Ô∏è Website Crawler API

The **Website Crawler API** allows developers to programmatically crawl websites and access structured metadata via four simple endpoints. This API gives you clean JSON responses and real-time crawl updates. The JSON/structured response generated by the /crawl/cwdata endpoint can be used for a variety of purpose. For example, as the data is JSON/LLM ready format, you can use it to train an AI model, use it for creating chatbots, audit websites, etc.

---

## üîê Authentication

To use the API, you'll need an **API Key**.

**How to get one:**
1. Visit [websitecrawler.org](https://www.websitecrawler.org)
2. Create an account or log in
3. Go to the **Settings** page to generate your API key
---

##  Example usage of the Java Library
The following code demonstrates the basic use of Website Crawler API. In the program, we will submit a URL to websitecrawler.org for it to crawl and retrive the status, currentURL and the data via the API.

###How to use the library?

Download the jar file WebsiteCrawlerSDK-Java-1.0.jar and add it as a dependency in your java project. Create the WebsiteCrawlerConfig object as shown in the code. Pass the WebsiteCrawlerConfig object to WebsiteCrawlerClient. Use the WebsiteCrawlerConfig object to call the methods.

```java

package wc.WebsiteCrawlerAPIUsageDemo;

import wc.websitecrawlersdk.WebsiteCrawlerClient;
import wc.websitecrawlersdk.WebsiteCrawlerConfig;

/**
 *
 * @author Pramod
 */
public class WebsiteCrawlerAPIUsageDemo {

    public static void main(String[] args) throws InterruptedException {
        String status;
        String currenturl;
        String data;
        WebsiteCrawlerConfig cfg = new WebsiteCrawlerConfig(YOUR_API_KEY); //replace YOUR_API_KEY with your api key
        WebsiteCrawlerClient client = new WebsiteCrawlerClient(cfg);

        client.submitUrlToWebsiteCrawler(URL, LIMIT); //replace "URL" with the URL you want Websitecrawler.org to crawl and the number of URLs
        boolean taskStatus;
        while (true) {
            taskStatus = client.getTaskStatus();
            System.out.println(taskStatus + "<<task status");
            Thread.sleep(9000);
            if (taskStatus == true) {
                status = client.getCrawlStatus();
                currenturl = client.getCurrentURL();
                data = client.getcwData();
                System.out.println("Crawl status::");
                if (status != null) {
                    System.out.println(status);
                }
                if (status != null && status.equals("Crawling")) {
                    System.out.println("Current URL::" + currenturl);
                }
                if (status != null && status.equals("Completed!")) {
                    System.out.println("Task has been completed.. closing the while loop");
                    if (data != null) {
                        System.out.println("Json Data::" + data);
                        Thread.sleep(20000);
                        break;
                    }
                }

            }
        }
        System.out.println("job over");
    }
}

```

## üåê Base URL

https://www.websitecrawler.org/api

---

## üì° Endpoints

### 1. `GET /crawl/start`

Pass your key as a query parameter (`key`) in all requests.


Initiate a new crawl for a given domain. 

- **Query Parameters**:
  - `url` (string, required): Target website (e.g. `example.com`) i.e. a non redirecting main URL of the website.
  - `limit` (integer, required): Max pages to crawl (free tier is resticted to 100)
  - `key` (string, required): Your API Key

- **Sample Request to initiate crawling**:

- https://www.websitecrawler.org/api/crawl/start?url=wptls.com&limit=50&key=YOUR_API_KEY

- - **Sample Response 1**:
```json
{
  "status": "Crawling"
}

```
- - **Sample Response 2**:

```json
{
  "status": "Completed!"
} 
```
---

### 2. `GET /crawl/cwdata`

Retrieve the structured crawl output once crawling has completed.

- **Query Parameters**:
  - `url` (string, required): Target website (e.g. `example.com`)
  - `key` (string, required): Your API Key

- - **Sample Response**:
```json
{
  "status": [
    {
      "tt": "WPTLS - WordPress Plugins, themes and related services",
      "np": "12",
      "h1": "",
      "nw": "534",
      "h2": "Why learn HTML when there is WordPress?",
      "h3": "",
      "h4": "",
      "h5": "",
      "atb": "Why learn HTML when there is WordPress?",
      "sc": "200",
      "md": "Reviews, comparison, and collection of top WordPress themes, plugins, related services, and useful WP tips.",
      "elsc": "",
      "textCN": "Websitedata.",
      "d": "",
      "mr": "follow, index",
      "pname": "wptls.com",
      "al": "",
      "cn": "https://wptls.com/",
      "kw": "",
      "url": "https://wptls.com",
      "at": "",
      "external_links": "https://www.facebook.com/wptls",
      "tm": "96",
      "image_links": "https://wptls.com/wp-content/uploads/2021/12/cropped-wptls-logo.png | https://wptls.com/wp-content/uploads/2021/12/cropped-wptls-logo.png | https://wptls.com/wp-content/uploads/2024/02/Spaceship-768x378.jpg | https://wptls.com/wp-content/uploads/2023/12/AdSense-768x612.png | https://wptls.com/wp-content/uploads/2023/12/Exabytes-768x375.jpg | https://wptls.com/wp-content/uploads/2023/10/HTML-768x112.jpg | https://wptls.com/wp-content/uploads/2023/10/Cloudflare-add-site-768x363.png | https://wptls.com/wp-content/uploads/2023/01/Google-Trends-768x363.webp | https://wptls.com/wp-content/uploads/2022/11/Twenty-Twenty-Three-768x351.webp | https://wptls.com/wp-content/uploads/2022/11/Broken-Link-Checker-768x223.webp | https://wptls.com/wp-content/uploads/2022/11/wordpress_logo.webp | https://wptls.com/wp-content/uploads/2022/11/footer-css-768x327.webp",
      "internal_links": "https://wptls.com/why-learn-html-when-there-is-wordpress/ | https://wptls.com/customize-footer-wordpress/",
      "nofollow_links": ""
    }
  ]
}
```
### 3. `GET /crawl/currentURL`

Get the last crawled/processed URL

- **Query Parameters**:
  - `url` (string, required): Target website (e.g. `example.com`) i.e. a non redirecting main URL of the website.
  - `key` (string, required): Your API Key

- **Sample Request to get the last crawled/processed URL**:

- https://www.websitecrawler.org/api/crawl/currentURL?url=wptls.com&key=YOUR_API_KEY

- - **Sample Response**:
```json
{
  "currentURL": "https://wptls.com"
}
```

### 4. `GET /crawl/clear`

Clear the previous job in case you want to rerun the crawler.

- **Query Parameters**:
  - `url` (string, required): Target website (e.g. `example.com`) i.e. a non redirecting main URL of the website.
  - `key` (string, required): Your API Key

- **Sample Request to get the last crawled/processed URL**:

- https://www.websitecrawler.org/api/crawl/clear?url=wptls.com&key=YOUR_API_KEY

- - **Sample Response**:
```json
{
  "clearStatus": "Job cannot be cleared as the URL of the entered website is being crawled."
}
```

## üß© Integration Example: XML Sitemap Generator

This section highlights how the [`XML-Sitemap-Generator`](https://github.com/pc8544/XML-Sitemap-Generator) project uses the `websitecrawler.org` API to automate XML sitemap generation.

### üîÑ Integration Workflow

The following steps outline the flow between the Website Crawler API and the sitemap generation logic:

1. **Start Crawling**
   - Use the `crawl/start` endpoint to initiate crawling of your website:
     ```bash
     https://www.websitecrawler.org/api/crawl/start?url=example.com&limit=100&key=YOUR_API_KEY
     ```

2. **Fetch Crawled Data**
   - Once crawling is complete, retrieve data using:
     ```bash
     https://www.websitecrawler.org/api/crawl/cwdata?url=example.com&key=YOUR_API_KEY
     ```
   - Response includes structured metadata (titles, links, status codes, etc.) in JSON format.

3. **Process and Transform**
   - The XML Sitemap Generator parses the response and extracts valid URLs.

4. **Generate Sitemap**
   - The extracted URLs are then converted into a compliant `sitemap.xml` for SEO optimization and better search engine indexing.

### üìÇ Repository

Check out the full implementation here:  
üîó [`XML-Sitemap-Generator`](https://github.com/pc8544/XML-Sitemap-Generator)

---

For best results, ensure your API key is valid and your domain permits crawling.

##üëã Feedback & Support
Found a bug or need help? Open an issue or connect via [websitecrawler.org](https://www.websitecrawler.org)
